% Encoding: UTF-8

@InProceedings{KyriazisOikonomidisPantelerisEtAl2015a,
  author    = {\textbf{Kyriazis}, \textbf{Nikolaos} and Oikonomidis, Iason and Panteleris, Paschalis and Michel, Damien and Qammaz, Ammar and Makris, Alexandros and Tzevanidis, Konstantinos and Douvantzis, Petros and Roditakis, Konstantinos and Argyros, Antonis A},
  title     = {A Generative Approach to Tracking Hands and Their Interaction with Objects},
  booktitle = {Man-Machine Interactions 4 - International Conference on Man-Machine Interactions (ICMMI 2015)},
  year      = {2015},
  pages     = {19--28},
  address   = {Kocierz, Poland},
  month     = oct,
  publisher = {Springer},
  abstract  = {Markerless 3D tracking of hands in action or in interaction with objects provides rich information that can be used to interpret a number of human activities. In this paper, we review a number of relevant methods we have proposed. All of them focus on hands, objects and their interaction and follow a generative approach. The major strength of such an approach is the straightforward fashion in which arbitrarily complex priors can be easily incorporated towards solving the tracking problem and their capability to generalize to greater and/or different domains. The proposed generative approach is implemented in a single, unified computational framework.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icmmi/KyriazisOPMQMTD15},
  date      = {2016-01-01},
  doi       = {10.1007/978-3-319-23437-3_2},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2015_10_ICMMI_keynote.pdf:PDF},
  isbn      = {978-3-319-23436-6},
  projects  = {ROBOHOW,WEARHAP},
  timestamp = {Thu, 15 Oct 2015 15:34:40 +0200},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2015_10_ICMMI_keynote.pdf},
}

@InProceedings{MakrisKyriazisArgyros2015a,
  author    = {Makris, Alexandros and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {Hierarchical particle filtering for 3d hand tracking},
  booktitle = {IEEE Computer Vision and Pattern Recognition Workshops (HANDS 2015 - CVPRW 2015)},
  year      = {2015},
  pages     = {8--17},
  address   = {Boston, USA},
  month     = jun,
  publisher = {IEEE},
  abstract  = {We present a fast and accurate 3D hand tracking method which relies on RGB-D data. The method follows a model based approach using a hierarchical particle filter variant to track the model's state. The filter estimates the probability density function of the state's posterior. As such, it has increased robustness to observation noise and compares favourably to existing methods that can be trapped in local minima resulting in track loses. The data likelihood term is calculated by measuring the discrepancy between the rendered 3D model and the observations. Extensive experiments with real and simulated data show that hand tracking is achieved at a frame rate of 90fps with less that 10mm average error using a GPU implementation, thus comparing favourably to the state of the art in terms of both speed and tracking accuracy.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cvpr/MakrisKA15},
  doi       = {10.1109/CVPRW.2015.7301343},
  file      = {C:\\antonis\\professional\\public_html\\mypapers\\2015_06_HANDS_hmf.pdf:PDF},
  keywords  = {graphics processing units;image colour analysis;object tracking;particle filtering (numerical methods);probability;rendering (computer graphics);solid modelling;3D hand tracking method;GPU implementation;RGB-D data;data likelihood term;hierarchical particle filtering;observation noise;probability density function;rendered 3D model;tracking accuracy;Bayes methods},
  projects  = {WEARHAP, ROBOHOW},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2015_06_HANDS_hmf.pdf},
  videolink = {https://youtu.be/DR8YUOAM3QM},
}

@InProceedings{KyriazisArgyros2014a,
  author    = {\textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {Scalable 3D Tracking of Multiple Interacting Objects},
  booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR 2014)},
  year      = {2014},
  pages     = {3430--3437},
  address   = {Columbus, Ohio, USA},
  month     = jun,
  publisher = {IEEE},
  abstract  = {We consider the problem of tracking multiple interacting objects in 3D, using RGBD input and by considering a hypothesize-and-test approach. Due to their interaction, objects to be tracked are expected to occlude each other in the field of view of the camera observing them. A naive approach would be to employ a Set of Independent Trackers (SIT) and to assign one tracker to each object. This approach scales well with the number of objects but fails as occlusions become stronger due to their disjoint consideration. The solution representing the current state of the art employs a single Joint Tracker (JT) that accounts for all objects simultaneously. This directly resolves ambiguities due to occlusions but has a computational complexity that grows geometrically with the number of tracked objects. We propose a middle ground, namely an Ensemble of Collaborative Trackers (ECT), that combines best traits from both worlds to deliver a practical and accurate solution to the multi-object 3D tracking problem. We present quantitative and qualitative experiments with several synthetic and real world sequences of diverse complexity. Experiments demonstrate that ECT manages to track far more complex scenes than JT at a computational time that is only slightly larger than that of SIT.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cvpr/KyriazisA14},
  doi       = {10.1109/CVPR.2014.438},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2014_06_cvpr_ect.pdf:PDF},
  keywords  = {cameras;computational complexity;object tracking;ECT;RGBD input;SIT;camera;computational complexity;computational time;disjoint consideration;ensemble of collaborative trackers;hypothesize-and-test approach;middle ground;multiobject 3D tracking problem;multiple interacting object tracking;naive approach;occlusions;scalable 3D tracking;set of independent trackers;Accuracy;Collaboration;Joints;Linear programming;Optimization;Three-dimensional displays;Tracking;3d;efficient;interaction;joint;occlusions;optimization;tracking},
  projects  = {ROBOHOW, WEARHAP},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2014_06_cvpr_ect.pdf},
  videolink = {https://youtu.be/SCOtBdhDMKg},
}

@InProceedings{DouvantzisOikonomidisKyriazisEtAl2013a,
  author    = {Douvantzis, Petros and Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {Dimensionality Reduction for Efficient Single Frame Hand Pose Estimation},
  booktitle = {International Conference on Computer Vision Systems (ICVS 2013)},
  year      = {2013},
  pages     = {143--152},
  address   = {St. Petersburg, Russia},
  month     = jul,
  publisher = {Springer},
  abstract  = {Model based approaches for the recovery of the 3D position, orientation and full articulation of the human hand have a number of attractive properties. One bottleneck towards their practical exploitation is their computational cost. To a large extent, this is determined by the large dimensionality of the problem to be solved. In this work we exploit the fact that the parametric joints space representing hand configurations is highly redundant. Thus, we employ Principal Component Analysis (PCA) to learn a lower dimensional space that describes compactly and effectively the human hand articulation. The reduced dimensionality of the resulting space leads to a simpler optimization problem, so model-based approaches require less computational effort to solve it. Experiments demonstrate that the proposed approach achieves better accuracy in hand pose recovery compared to a state of the art baseline method using only 1/4 of the latterâ€™s computational budget.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icvs/DouvantzisOKA13},
  date      = {2013-01-01},
  doi       = {10.1007/978-3-642-39402-7_15},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2013_07_icvs_dimreduction.pdf:PDF},
  isbn      = {978-3-642-39401-0},
  projects  = {WEARHAP},
  timestamp = {Fri, 12 Jul 2013 15:42:00 +0200},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2013_07_icvs_dimreduction.pdf},
}

@InProceedings{KyriazisArgyros2013a,
  author    = {\textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis},
  booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR 2013)},
  year      = {2013},
  pages     = {9--16},
  address   = {Portland, Oregon, USA},
  month     = jun,
  publisher = {IEEE},
  abstract  = {In several hand-object(s) interaction scenarios, the change in the objects' state is a direct consequence of the hand's motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cvpr/KyriazisA13},
  doi       = {10.1109/CVPR.2013.9},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2013_06_cvpr_singleactor.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {image motion analysis;object tracking;3D hand-object interaction;3D pose;Newtonian dynamics;direct consequence;hand motion;model-based 3D tracking;observability issues;physically plausible 3D scene tracking;single actor hypothesis;straightforward representation;table-top scene;Cameras;Dynamics;Optimization;Shape;Solid modeling;Three-dimensional displays;Tracking;3d;actor;hand;hypothesis;multi;physically;physics;plausible;single;tracking},
  projects  = {ROBOHOW},
  timestamp = {Thu, 31 Jul 2014 17:22:01 +0200},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2013_06_cvpr_singleactor.pdf},
  videolink = {https://youtu.be/0RCsQPXeHRQ},
}

@InProceedings{PatelEkKyriazisEtAl2013a,
  author    = {Patel, Mitesh and Ek, Carl Henrik and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A and Mir{\'{o}}, Jaime Valls and Kragic, Danica},
  title     = {Language for learning complex human-object interactions},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA 2013)},
  year      = {2013},
  pages     = {4997--5002},
  address   = {Karlsruhe, Germany},
  month     = may,
  publisher = {IEEE},
  abstract  = {In this paper we use a Hierarchical Hidden Markov Model (HHMM) to represent and learn complex activities/task performed by humans/robots in everyday life. Action primitives are used as a grammar to represent complex human behaviour and learn the interactions and behaviour of human/robots with different objects. The main contribution is the use of a probabilistic model capable of representing behaviours at multiple levels of abstraction to support the proposed hypothesis. The hierarchical nature of the model allows decomposition of the complex task into simple action primitives. The framework is evaluated with data collected for tasks of everyday importance performed by a human user.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icra/PatelEKAMK13},
  doi       = {10.1109/ICRA.2013.6631291},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2013_05_icra_mitesh.pdf:PDF},
  issn      = {1050-4729},
  keywords  = {grammars;hidden Markov models;human-robot interaction;learning (artificial intelligence);natural language processing;probability;HHMM;action primitives;complex activities-task learning;complex human behaviour;complex human-object interaction learning;grammar;hierarchical hidden Markov model;language;probabilistic model;Abstracts;Accuracy;Data models;Hidden Markov models;Joints;Probabilistic logic;Robots},
  projects  = {ROBOHOW, GRASP},
  timestamp = {Tue, 22 Oct 2013 08:50:26 +0200},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2013_05_icra_mitesh.pdf},
}

@InProceedings{SongKyriazisOikonomidisEtAl2013a,
  author       = {Song, Dan and \textbf{Kyriazis}, \textbf{Nikolaos} and Oikonomidis, Iason and Chavdar Papazov and Argyros, Antonis A and Burschka, Darius and Kragic, Danica},
  title        = {Predicting human intention in visual observations of hand/object interactions},
  booktitle    = {IEEE International Conference on Robotics and Automation (ICRA 2013)},
  year         = {2013},
  pages        = {1608--1615},
  address      = {Karlsruhe, Germany},
  month        = may,
  organization = {IEEE},
  publisher    = {IEEE},
  abstract     = {The main contribution of this paper is a probabilistic method for predicting human manipulation intention from image sequences of human-object interaction. Predicting intention amounts to inferring the imminent manipulation task when human hand is observed to have stably grasped the object. Inference is performed by means of a probabilistic graphical model that encodes object grasping tasks over the 3D state of the observed scene. The 3D state is extracted from RGB-D image sequences by a novel vision-based, markerless hand-object 3D tracking framework. To deal with the high-dimensional state-space and mixed data types (discrete and continuous) involved in grasping tasks, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Experimentation showed that the model trained on simulated data can provide a potent basis for accurate goal-inference with partial and noisy observations of actual real-world demonstrations. We also show a grasp selection process, guided by the inferred human intention, to illustrate the use of the system for goal-directed grasp imitation.},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/conf/icra/SongKOPABK13},
  doi          = {10.1109/ICRA.2013.6630785},
  file         = {c:\\antonis\\professional\\public_html\\mypapers\\2013_05_icra_song.pdf:PDF},
  issn         = {1050-4729},
  keywords     = {behavioural sciences computing;image sequences;inference mechanisms;object tracking;self-organising feature maps;vector quantisation;RGB-D image sequences;generative vector quantization method;goal-directed grasp imitation;goal-inference;grasp selection process;high-dimensional state-space data;human manipulation intention prediction;markerless hand-object 3D tracking framework;mixed data types;mixture models;object grasping tasks encoding;probabilistic graphical model;scene 3D state extraction;self-organizing maps;vision-based hand-object 3D tracking framework;visual hand-object interaction observations;Bayes methods;Computational modeling;Data models;Grasping;Robot sensing systems;Three-dimensional displays},
  projects     = {GRASP},
  timestamp    = {Mon, 21 Oct 2013 16:10:25 +0200},
  url          = {http://users.ics.forth.gr/~argyros/mypapers/2013_05_icra_song.pdf},
}

@InProceedings{OikonomidisKyriazisArgyros2012a,
  author    = {Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {Tracking the articulated motion of two strongly interacting hands},
  booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR 2012)},
  year      = {2012},
  pages     = {1862--1869},
  address   = {Providence, Rhode Island, USA},
  month     = jun,
  publisher = {IEEE},
  abstract  = {We propose a method that relies on markerless visual observations to track the full articulation of two hands that interact with each-other in a complex, unconstrained manner. We formulate this as an optimization problem whose 54-dimensional parameter space represents all possible configurations of two hands, each represented as a kinematic structure with 26 Degrees of Freedom (DoFs). To solve this problem, we employ Particle Swarm Optimization (PSO), an evolutionary, stochastic optimization method with the objective of finding the two-hands configuration that best explains observations provided by an RGB-D sensor. To the best of our knowledge, the proposed method is the first to attempt and achieve the articulated motion tracking of two strongly interacting hands. Extensive quantitative and qualitative experiments with simulated and real world image sequences demonstrate that an accurate and efficient solution of this problem is indeed feasible.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cvpr/OikonomidisKA12},
  doi       = {10.1109/CVPR.2012.6247885},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2012_06_cvpr_twohands.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {evolutionary computation;image motion analysis;object tracking;particle swarm optimisation;stochastic programming;54-dimensional parameter space;PSO;RGB-D sensor;articulated motion tracking;evolutionary stochastic optimization;interacting hands;kinematic structure;particle swarm optimization;Computational modeling;Humans;Joints;Optimization;Skin;Tracking;Visualization},
  projects  = {GRASP,ROBOHOW},
  timestamp = {Tue, 25 Nov 2014 17:05:17 +0100},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2012_06_cvpr_twohands.pdf},
  videolink = {https://youtu.be/e3G9soCdIbc},
}

@InProceedings{KyriazisOikonomidisArgyros2011a,
  author        = {\textbf{Kyriazis}, \textbf{Nikolaos} and Oikonomidis, Iason and Argyros, Antonis A},
  title         = {Binding Computer Vision to Physics Based Simulation: The Case Study of a Bouncing Ball},
  booktitle     = {British Machine Vision Conference (BMVC 2011)},
  year          = {2011},
  pages         = {1--11},
  address       = {Dundee, UK},
  publisher     = {BMVA},
  __markedentry = {[argyros:]},
  abstract      = {A dynamic scene and, therefore, its visual observations are invariably determined by the laws of physics.   We demonstrate an illustrative case where physical explanation, as a vision prior,  is not a commodity but a necessity.   By considering the problem of ball motion estimation we show how physics-based simulation in conjunction with visual processes can lead to the reduction of the visual input required to infer physical attributes of the observed world.  Even further, we show that the proposed methodology manages to reveal certain physical attributes of the observed scene that are difficult or even impossible to extract by other means.  A series of experiments on synthetic data as well as experiments with image sequences of an actual ball, support the validity of the proposed approach. The use of generic tools and the top-down nature of the proposed approach make it general enough to be a likely candidate for handling even more complex problems in larger contexts.},
  bibsource     = {dblp computer science bibliography, http://dblp.org},
  biburl        = {http://dblp.uni-trier.de/rec/bib/conf/bmvc/KyriazisOA11},
  doi           = {10.5244/C.25.43},
  file          = {c:\\antonis\\professional\\public_html\\mypapers\\2011_09_bmvc_bouncing_ball.pdf:PDF},
  projects      = {GRASP},
  timestamp     = {Wed, 24 Apr 2013 17:19:07 +0200},
  url           = {http://users.ics.forth.gr/~argyros/mypapers/2011_09_bmvc_bouncing_ball.pdf},
  videolink     = {https://youtu.be/Lr5wq5It4io},
}

@InProceedings{OikonomidisKyriazisArgyros2011b,
  author       = {Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title        = {Efficient model-based 3D tracking of hand articulations using Kinect},
  booktitle    = {British Machine Vision Conference (BMVC 2011)},
  year         = {2011},
  volume       = {1},
  number       = {2},
  pages        = {1--11},
  address      = {Dundee, UK},
  organization = {BMVA Press},
  publisher    = {BMVA},
  abstract     = {We present a novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor.  We treat this as an optimization problem, seeking for the hand model parameters that minimize the discrepancy between the appearance and 3D structure of hypothesized instances of a hand model and actual hand observations. This optimization problem is effectively solved using a variant of Particle Swarm Optimization (PSO). The proposed method does not require special markers and/or a complex image acquisition setup.  Being model based, it provides continuous solutions to the problem of tracking hand articulations.  Extensive experiments with a prototype GPU-based implementation of the proposed method demonstrate that accurate and robust 3D tracking of hand articulations can be achieved in near real-time (15Hz).},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/conf/bmvc/OikonomidisKA11},
  doi          = {10.5244/C.25.101},
  file         = {c:\\antonis\\professional\\public_html\\mypapers\\2011_09_bmvc_kinect_hand_tracking.pdf:PDF},
  projects     = {GRASP},
  timestamp    = {Wed, 24 Apr 2013 17:19:07 +0200},
  url          = {http://users.ics.forth.gr/~argyros/mypapers/2011_09_bmvc_kinect_hand_tracking.pdf},
  videolink    = {https://youtu.be/Fxa43qcm1C4},
}

@InProceedings{OikonomidisKyriazisArgyros2011a,
  author    = {Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {Full {DOF} tracking of a hand interacting with an object by modeling occlusions and physical constraints},
  booktitle = {IEEE International Conference on Computer Vision (ICCV 2011)},
  year      = {2011},
  pages     = {2088--2095},
  address   = {Barcelona, Spain},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Due to occlusions, the estimation of the full pose of a human hand interacting with an object is much more challenging than pose recovery of a hand observed in isolation. In this work we formulate an optimization problem whose solution is the 26-DOF hand pose together with the pose and model parameters of the manipulated object. Optimization seeks for the joint hand-object model that (a) best explains the incompleteness of observations resulting from occlusions due to hand-object interaction and (b) is physically plausible in the sense that the hand does not share the same physical space with the object. The proposed method is the first that solves efficiently the continuous, full-DOF, joint hand-object tracking problem based solely on markerless multicamera input. Additionally, it is the first to demonstrate how hand-object interaction can be exploited as a context that facilitates hand pose estimation, instead of being considered as a complicating factor. Extensive quantitative and qualitative experiments with simulated and real world image sequences as well as a comparative evaluation with a state-of-the-art method for pose estimation of isolated hands, support the above findings.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/iccv/OikonomidisKA11},
  doi       = {10.1109/ICCV.2011.6126483},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2011_11_iccv_hope.pdf:PDF},
  issn      = {1550-5499},
  keywords  = {object tracking;optimisation;pose estimation;solid modelling;full DOF tracking;hand pose estimation;hand-object interaction;hand-object tracking;occlusions modeling;optimization;physical constraints modeling;Cameras},
  projects  = {GRASP},
  timestamp = {Thu, 19 Jan 2012 18:05:15 +0100},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2011_11_iccv_hope.pdf},
  videolink = {https://youtu.be/N3ffgj1bBGw},
}

@InProceedings{TzevanidisZabulisSarmisEtAl2010a,
  author    = {Tzevanidis, Konstantinos and Zabulis, Xenophon and Sarmis, Thomas and Koutlemanis, Panayotis and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {From Multiple Views to Textured 3D Meshes: {A} GPU-Powered Approach},
  booktitle = {European Conference on Computer Vision Workshops (CVGPU 2010 - ECCVW 2010)},
  year      = {2010},
  pages     = {384--397},
  address   = {Heraklion, Crete, Greece},
  month     = sep,
  publisher = {Springer},
  abstract  = {We present work on exploiting modern graphics hardware towards the real-time production of a textured 3D mesh representation of a scene observed by a multicamera system. The employed computational infrastructure consists of a network of four PC workstations each of which is connected to a pair of cameras. One of the PCs is equipped with a GPU that is used for parallel computations. The result of the processing is a list of texture mapped triangles representing the reconstructed surfaces. In contrast to previous works, the entire processing pipeline (foreground segmentation, 3D reconstruction, 3D mesh computation, 3D mesh smoothing and texture mapping) has been implemented on the GPU. Experimental results demonstrate that an accurate, high resolution, texture-mapped 3D reconstruction of a scene observed by eight cameras is achievable in real time.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/eccv/TzevanidisZSKKA10},
  doi       = {10.1007/978-3-642-35740-4_30},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2010_09_cvgpu_3Dreconstruction.pdf:PDF},
  projects  = {GRASP,AMIPROJ},
  timestamp = {Mon, 03 Dec 2012 13:34:31 +0100},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2010_09_cvgpu_3Dreconstruction.pdf},
  videolink = {https://youtu.be/n0KC7wL_D_Q},
}

@InProceedings{OikonomidisKyriazisArgyros2010a,
  author    = {Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title     = {Markerless and Efficient 26-DOF Hand Pose Recovery},
  booktitle = {Asian Conference on Computer Vision (ACCV 2010)},
  year      = {2010},
  pages     = {744--757},
  address   = {Queenstown, New Zealand},
  month     = apr,
  publisher = {Springer},
  abstract  = {We present a novel method that, given a sequence of synchronized views of a human hand, recovers its 3D position, orientation and full articulation parameters. The adopted hand model is based on properly selected and assembled 3D geometric primitives. Hypothesized configurations/poses of the hand model are projected to different camera views and image features such as edge maps and hand silhouettes are computed. An objective function is then used to quantify the discrepancy between the predicted and the actual, observed features. The recovery of the 3D hand pose amounts to estimating the parameters that minimize this objective function which is performed using Particle Swarm Optimization. All the basic components of the method (feature extraction, objective function evaluation, optimization process) are inherently parallel. Thus, a GPU-based implementation achieves a speedup of two orders of magnitude over the case of CPU processing. Extensive experimental results demonstrate qualitatively and quantitatively that accurate 3D pose recovery of a hand can be achieved robustly at a rate that greatly outperforms the current state of the art.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/accv/OikonomidisKA10},
  date      = {2011-01-01},
  doi       = {10.1007/978-3-642-19318-7_58},
  file      = {c:\\antonis\\professional\\public_html\\mypapers\\2010_11_ACCV_3Dhandpose.pdf:PDF},
  isbn      = {978-3-642-19317-0},
  projects  = {GRASP},
  timestamp = {Mon, 21 Feb 2011 13:01:36 +0100},
  url       = {http://users.ics.forth.gr/~argyros/mypapers/2010_11_ACCV_3Dhandpose.pdf},
}

@Inproceedings{KyriazisArgyros2015a,
  author =    {\textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title =     {3D Tracking of Hands Interacting with Several Objects},
  booktitle = {IEEE International Conference on Computer Vision Workshops (OUI 2015 - ICCVW 2015)},
  year =      {2015},
  address =   {Santiago, Chile},
  month =     nov,
  publisher = {IEEE},
  url =       {http://users.ics.forth.gr/~argyros/mypapers/2015_12_OUI_mbv.pdf},
  file =      {c:\\antonis\\professional\\public_html\\mypapers\\2015_12_OUI_mbv.pdf:PDF},
  projects =  {ROBOHOW,WEARHAP},
  videolink = {https://youtu.be/SCOtBdhDMKg}
}

@Inproceedings{PantelerisKyriazisArgyros2015b,
  author =    {Panteleris, Paschalis and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title =     {Recovering 3D models of manipulated objects through 3D tracking of hand-object interaction},
  booktitle = {IEEE International Conference on Computer Vision Workshops (OUI 2015 - ICCVW 2015)},
  year =      {2015},
  address =   {Santiago, Chile},
  month =     nov,
  publisher = {IEEE},
  url =       {http://users.ics.forth.gr/~argyros/mypapers/2015_12_OUI_padeler.pdf},
  file =      {c:\\antonis\\professional\\public_html\\mypapers\\2015_12_OUI_padeler.pdf:PDF},
  projects =  {ROBOHOW},
  videolink = {https://youtu.be/9r43PtJ0Fwg}
}

@Inproceedings{PantelerisKyriazisArgyros2015a,
  author =    {Panteleris, Paschalis and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title =     {3D Tracking of Human Hands in Interaction with Unknown Objects},
  booktitle = {British Machine Vision Conference (BMVC 2015)},
  year =      {2015},
  pages =     {123--1},
  address =   {Swansea, UK},
  month =     sep,
  publisher = {BMVA},
  url =       {http://users.ics.forth.gr/~argyros/mypapers/2015_09_BMVC_unknownobjects.pdf},
  abstract =  {The analysis and the understanding of object manipulation scenarios based on computer vision techniques can be greatly facilitated if we can gain access to the full articulation of the manipulating hands and the 3D pose of the manipulated objects. Currently, there exist methods for tracking hands in interaction with objects whose 3D models are known.  There are also methods that can reconstruct 3D models of objects that are partially observable in each frame of a sequence. However, to the best of our knowledge, no method can track hands in interaction with unknown objects.  In this paper we propose such a method.  Experimental results show that hand tracking can be achieved with an accuracy that is comparable to the one obtained by methods that assume knowledge of the object models. Additionally, as a by-product, the proposed method delivers accurate 3D models of the manipulated objects.},
  doi =       {https://dx.doi.org/10.5244/C.29.123},
  file =      {c:\\antonis\\professional\\public_html\\mypapers\\2015_09_BMVC_unknownobjects.pdf:PDF},
  projects =  {ROBOHOW},
  videolink = {https://youtu.be/9r43PtJ0Fwg}
}

@Inproceedings{QammazKyriazisArgyros2015a,
  author =       {Qammaz, Ammar and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title =        {Boosting the Performance of Model-based 3D Tracking by Employing Low Level Motion Cues},
  booktitle =    {British Machine Vision Conference (BMVC 2015)},
  year =         {2015},
  pages =        {144--1},
  address =      {Swansea, UK},
  month =        sep,
  organization = {BMVA Press},
  publisher =    {BMVA},
  url =          {http://users.ics.forth.gr/~argyros/mypapers/2015_09_BMVC_fect.pdf},
  abstract =     {3D tracking of objects and hands in an object manipulation scenario is a very interesting computer vision problem with a wide variety of applications ranging from consumer electronics to robotics and medicine. Recent advances in this research topic allow for 3D tracking of complex scenarios involving bimanual manipulation of several rigid objects using commodity hardware and with high accuracy. The problem with these approaches is that they treat tracking as a search problem whose dimensionality increases with the number of objects in the scene.  This fact typically limits the number of the tracked objects and/or the processing framerate. In this paper we present a method that utilizes simple low level motion cues for dynamically assigning computational resources to parts of the scene where they are actually required.  In a series of experiments, we show that this simple idea improves tracking performance dramatically at a cost of only a minor degradation of tracking accuracy.},
  doi =          {https://dx.doi.org/10.5244/C.29.144},
  file =         {c:\\antonis\\professional\\public_html\\mypapers\\2015_09_BMVC_fect.pdf},
  projects =     {ROBOHOW,WEARHAP},
  videolink =    {https://youtu.be/nPru6PpWrK4}
}

@Inproceedings{OikonomidisKyriazisTzevanidisEtAl2013a,
  author =    {Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Tzevanidis, Konstantinos and Argyros, Antonis A},
  title =     {Tracking hand articulations: Relying on 3D visual hulls versus relying on multiple 2D cues},
  booktitle = {IEEE International Symposium on Ubiquitous Virtual Reality (ISUVR 2013)},
  year =      {2013},
  pages =     {7--10},
  month =     jul,
  publisher = {IEEE},
  url =       {http://users.ics.forth.gr/~argyros/mypapers/2013_07_isuvr_handvisualhull.pdf},
  abstract =  {We present a method for articulated hand tracking that relies on visual input acquired by a calibrated multicamera system. A state-of-the-art result on this problem has been presented in [12]. In that work, hand tracking is formulated as the minimization of an objective function that quantifies the discrepancy between a hand pose hypothesis and the observations. The objective function treats the observations from each camera view in an independent way. We follow the same general optimization framework but we choose to employ the visual hull [10] as the main observation cue, which results from the integration of information from all available views prior to optimization. We investigate the behavior of the resulting method in extensive experiments and in comparison with that of [12]. The obtained results demonstrate that for low levels of noise contamination, regardless of the number of cameras, the two methods perform comparably. The situation changes when noisy observations or as few as two cameras with short baselines are employed. In these cases, the proposed method is more accurate than that of [12]. Thus, the proposed method is preferable in real-world scenarios with noisy observations obtained from easy-to-deploy, stereo camera setups.},
  adress =    {Daejeon, South Korea},
  doi =       {10.1109/ISUVR.2013.13},
  file =      {c:\\antonis\\professional\\public_html\\mypapers\\2013_07_isuvr_handvisualhull.pdf:PDF},
  keywords =  {cameras;image denoising;minimisation;object tracking;palmprint recognition;stereo image processing;3D visual hulls;articulated hand tracking;calibrated multicamera system;camera view;hand pose hypothesis;multiple 2D cues;noise contamination;noisy observations;objective function minimization;optimization framework;stereo camera;visual input;Cameras;Computational modeling;Linear programming;Noise;Skin;Three-dimensional displays;Visualization;3D hand tracking;visual hull},
  projects =  {ROBOHOW,WEARHAP}
}

@Inproceedings{KyriazisOikonomidisArgyros2012a,
  author =    {\textbf{Kyriazis}, \textbf{Nikolaos} and Oikonomidis, Iason and Argyros, Antonis A},
  title =     {A GPU-powered Computational Framework for Efficient 3D Model-based Vision},
  booktitle = {International Conference on Cognitive Systems (COGSYS 2012)},
  year =      {2012},
  address =   {Vienna, Austria},
  month =     feb,
  url =       {http://users.ics.forth.gr/~argyros/mypapers/2012_02_cogsys_kyriazis.pdf},
  file =      {c:\\antonis\\professional\\public_html\\mypapers\\2012_02_cogsys_kyriazis.pdf:PDF},
  projects =  {GRASP}
}

@Inproceedings{OikonomidisKyriazisArgyros2012b,
  author =    {Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A},
  title =     {Efficient Model-based Tracking of the Articulated Motion of Hands},
  booktitle = {International Conference on Cognitive Systems (COGSYS 2012)},
  year =      {2012},
  address =   {Vienna, Austria},
  month =     feb,
  url =       {http://users.ics.forth.gr/~argyros/mypapers/2012_02_cogsys_oikonom.pdf},
  file =      {c:\\antonis\\professional\\public_html\\mypapers\\2012_02_cogsys_oikonom.pdf:PDF},
  projects =  {GRASP}
}

@article{pham2018hand,
  title={Hand-object contact force estimation from markerless visual tracking},
  author={Pham, Tu-Hoa and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis A and Kheddar, Abderrahmane},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2883--2896},
  year={2018},
  publisher={IEEE}
}

@inproceedings{qammaz2018distributed,
  title={Distributed Real-Time Generative 3D Hand Tracking using Edge GPGPU Acceleration},
  author={Qammaz, Ammar and Kosta, Sokol and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis},
  booktitle={Mobisys' 18-International Conference on Mobile Systems, Applications and Services},
  year={2018},
  organization={Association for Computing Machinery}
}

@Article{qammaz2018feasibility,
  author  = {Qammaz, Ammar and Kosta, Sokol and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis},
  title   = {On the Feasibility of Real-Time 3D Hand Tracking using Edge GPGPU Acceleration},
  journal = {arXiv preprint arXiv:1804.11256},
  year    = {2018},
}

@Article{bautembach2019faster,
  author  = {Bautembach, Dennis and Oikonomidis, Iason and \textbf{Kyriazis}, \textbf{Nikolaos} and Argyros, Antonis},
  title   = {Faster and Simpler SNN Simulation with Work Queues},
  journal = {arXiv preprint arXiv:1912.07423},
  year    = {2019},
}

@Comment{jabref-meta: databaseType:bibtex;}
